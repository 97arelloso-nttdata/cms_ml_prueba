<p align="left">
<img width=15% src="https://dai.lids.mit.edu/wp-content/uploads/2018/06/Logo_DAI_highres.png" alt=“DAI-Lab” />
<i>A project from Data to AI Lab at MIT.</i>
</p>

# CMS-ML

Machine Learning tools for CMS vibration data

## Overview

CMS-ML is a Python library for pre-processing and applying Machine
Learning on vibration data like the one generated by the CMS WIND systems.

It implements a collection of signal processing primitives and pipelines
that allow extracting *Feature Timeseries* from raw vibration data and
then apply Machine Learning Pipelines on them using [GreenGuard Project](
https://github.com/signals-dev/GreenGuard).

## Resources

* [Data Format](DATA_FORMAT.md).
* [CMS-ML folder structure](CMS_ML_FOLDER_STRUCTURE.md).
* [GreenGuard folder structure](https://github.com/signals-dev/GreenGuard/blob/master/DATA_FORMAT.md#folder-structure).

# Data Format

## Input

The input for CMS-ML is a table of FFT Timeseries, which contains the following 4 fields:

  * `turbine_id`: Unique identifier of the turbine to which this value relates.
  * `signal_id`: Unique identifier of the signal to which this value belongs.
  * `timestamp (datetime)`: Time at which this reading was taken.
  * `values (array)`: Array containing the values of the FFT spectrum of the signal at the
    indicated timestamp.

| turbine_id   | signal_id   | timestamp           |             values |
|--------------|-------------|---------------------|--------------------|
| T1           | S1          | 2001-01-01 00:00:00 |  [0.1, 1.5, 2.3..] |
| T1           | S1          | 2001-01-01 12:00:00 |  [0.0, 0.5, 1.2..] |
| T1           | S1          | 2001-01-02 00:00:00 |  [0.4, 3.3, 4.9..] |
| T1           | S1          | 2001-01-02 12:00:00 |  [0.1, 2.5, 5.5..] |
| T1           | S2          | 2001-01-01 00:00:00 |  [0.1, 1.2, 2.3..] |
| T1           | S2          | 2001-01-01 12:00:00 |  [1.0, 1.5, 2.3..] |
| T1           | S2          | 2001-01-02 00:00:00 |  [2.0, 1.5, 2.3..] |
| T1           | S2          | 2001-01-02 12:00:00 |  [3.5, 0.5, 2.3..] |
| ...          | ...         | ...                 |                ... |

Optionally, an arbitrary number of additional fields can be included as contextual metadata:

| turbine_id   | signal_id   | timestamp           |             values |  Measured RPM | ... |
|--------------|-------------|---------------------|--------------------|---------------|-----|
| T1           | S1          | 2001-01-01 00:00:00 |  [0.1, 1.5, 2.3..] |      1421.991 | ... |
| T1           | S1          | 2001-01-01 12:00:00 |  [0.0, 0.5, 1.2..] |       434.132 | ... |
| T1           | S1          | 2001-01-02 00:00:00 |  [0.4, 3.3, 4.9..] |      1021.210 | ... |
| T1           | S1          | 2001-01-02 12:00:00 |  [0.1, 2.5, 5.5..] |      1312.596 | ... |
| T1           | S2          | 2001-01-01 00:00:00 |  [0.1, 1.2, 2.3..] |      1219.249 | ... |
| T1           | S2          | 2001-01-01 12:00:00 |  [1.0, 1.5, 2.3..] |       850.810 | ... |
| T1           | S2          | 2001-01-02 00:00:00 |  [2.0, 1.5, 2.3..] |       995.543 | ... |
| T1           | S2          | 2001-01-02 12:00:00 |  [3.5, 0.5, 2.3..] |      4592.991 | ... |
| ...          | ...         | ...                 |                ... |           ... | ... |

## JSON data

Apart from the data format explained above, CMS-ML is also prepared to load and work with data
stored as a collection of JSON files. Further details about this format and how to extract the
FFT Timeseries from them can be found [here](DATA_FORMAT.md#JSON-Format).

# Install

## Requirements

**CMS-ML** has been developed and tested on [3.6 and 3.7](https://www.python.org/downloads/)

Also, although it is not strictly required, the usage of a [virtualenv](
https://virtualenv.pypa.io/en/latest/) is highly recommended in order to
avoid interfering with other software installed in the system in which
**CMS-ML** is run.

## Install from PyPI

After creating the virtualenv and activating it, we recommend using
[pip](https://pip.pypa.io/en/stable/) in order to install **CMS-ML**.

Note that, since the **CMS-ML** package is private, we will need to point
our `pip` command at the corresponding private PyPI instance and introduce
a valid username and password when requested.

```bash
pip install --extra-index-url https://pypi.dailab.ml/ cms-ml
```

This will pull and install the latest stable release from [our private PyPI
](https://pypi.dailab.ml/).

If you want to install from source or contribute to the project please
read the [Contributing Guide](CONTRIBUTING.rst).

## Docker usage

Alternatively, **CMS-ML** is prepared to be run inside a docker environment. Please check the
[docker documentation](docker/README.md) for details about how to run **CMS-ML** using docker.

# Quickstart

In this short tutorial we will guide you through a series of steps that will
help you get started with **CMS-ML**.

## Generate Demo Data

The first step will be to get some demo data to play with.

To do this, please execute the function `cms_ml.get_demo_data`.

```python3
from cms_ml import get_demo_data

data = get_demo_data()
```

This will return a `pandas.DataFrame` with the FFT timeseries format shown above:

```
turbine_id        signal_id           timestamp                                             values
      T001  Sensor1_signal1 2020-01-01 00:00:00  [8.544281799811039, -2.0076341772953614, 2.422...
      T001  Sensor1_signal1 2020-01-01 01:00:00  [-1.6941358909755944, 0.836503459891649, 1.027...
      T001  Sensor1_signal1 2020-01-01 02:00:00  [-3.277768425512229, -3.5145317121287913, -2.9...
      T001  Sensor1_signal1 2020-01-01 03:00:00  [-3.8445552569238655, 0.735052215433941, 1.722...
      T001  Sensor1_signal1 2020-01-01 04:00:00  [-2.7572893539605015, -2.13757922531778, -1.85...
```

## Define aggregation functions

In order to extract feature from the FFT time series we will need to apply some aggregation
functions to them.

In this case, we will apply the `mean` function from Numpy and the `rms` function defined
inside the module by creating a dictionary giving a name to each function:

```python3
import numpy as np

fft_aggregations = {
    'mean': np.mean,
}
```

## Extract Feature Timeseries

Once we have the aggregations defined, we can pass them to the `cms_ml.extract_cms_features`
function alongside our data:

```python3
from cms_ml import extract_cms_features

features = extract_cms_features(
    data,
    aggregations=fft_aggregations,
)
```

> :information_source: Optionally, instead of passing a `pandas.DataFrame` as data, you can pass
> the path to a CSV file and `cms_ml` will load it from the disk for you.

The output will be a table containing the extracted feature timeseries with the following format:

```
  turbine_id            signal_id           timestamp      value
0       T001  Sensor1_signal1_std 2020-01-01 00:00:00  10.275436
1       T001  Sensor1_signal1_std 2020-01-01 01:00:00   8.151766
2       T001  Sensor1_signal1_std 2020-01-01 02:00:00   9.344264
3       T001  Sensor1_signal1_std 2020-01-01 03:00:00   7.647820
4       T001  Sensor1_signal1_std 2020-01-01 04:00:00   8.711630
5        ...                  ...                 ...        ...
```

## Run a Machine Learning Pipeline

Once we have extracted the feature timeseries we are ready to learn a
Machine Learning model from them and make predictions.

### Load Target Times

The first step will be to load the demo target times:

```python3
from cms_ml import get_demo_target_times

target_times = get_demo_target_times()
```

### Select a Pipeline

Afterwards we will have to select one of the available Machine Learning
pipelines.

We can obtain the list of them using the `cms_ml.get_pipelines` function.

```python3
from cms_ml import get_pipelines

get_pipelines(pipeline_type='cms_ml')
```

which will return a list of pipeline names:

```
['cms_ml.apply_ifft_wav_var_lstm',
 'cms_ml.apply_kurtosis_lstm',
 'cms_ml.apply_var_lstm',
 'cms_ml.apply_wav_var_lstm',
 'cms_ml.normalize_dfs_1d_xgb_classifier']
```

For this quickstart we will use the pipeline named `cms_ml.normalize_dfs_1d_xgb_classifier`.

### Fit the Pipeline

Once we have selected our pipeline we can simply create a
`GreenGuardPipeline` instance and fit it to our data.

In this example, we will be fitting the pipeline on the first
40 rows of our target times and keep the other 10 for later
validation.

```python3
from greenguard import GreenGuardPipeline

train_target_times = target_times.iloc[:40]
test_target_times = target_times.iloc[40:]

pipeline = GreenGuardPipeline(
    'cms_ml.normalize_dfs_1d_xgb_classifier',
    metric='f1_macro',
    cv_splits=3,
)
pipeline.fit(train_target_times, features)
```

### Make predictions

Once the pipeline has been fitted to our data, we can use it to
make predictions on the rest of the data and evaluate how good
the predictions are.

```python3
from greenguard.metrics import f1_macro

X_test = test_target_times[['turbine_id', 'cutoff_time']]
y_test = test_target_times['target']
predictions = pipeline.predict(X_test, features)

f1_macro(y_test, predictions)
```
